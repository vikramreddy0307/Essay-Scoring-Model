{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.linear_model import LogisticRegressionCV as LogRegCV\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_regularized_scores(old_df):\n",
    "    new_df = old_df.copy()\n",
    "    new_df['std_score'] = new_df.groupby(['essay_set'])[['score']].apply(lambda x: (x - np.mean(x)) / (np.std(x)))\n",
    "    return new_df\n",
    "\n",
    "def create_regularization_data(old_df):\n",
    "    #getting the number of datasets\n",
    "    max_essay_set = max(old_df['essay_set'])\n",
    "    #list of the regularized values\n",
    "    regularization_data = []\n",
    "    for i in range(max_essay_set+1):\n",
    "        mean = np.mean((old_df[old_df['essay_set'] == i + 1])['score'])\n",
    "        std = np.std((old_df[old_df['essay_set'] == i + 1])['score'])\n",
    "        regularization_data.append([i + 1, mean, std])\n",
    "    return regularization_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_cols = ['essay_id', 'essay_set', 'essay', 'domain1_score', 'domain2_score']\n",
    "train_df = pd.read_csv('training_set_rel3.tsv', delimiter='\\t', usecols=train_cols,encoding='iso-8859-1')\n",
    "for i in range(train_df.shape[0]):\n",
    "    if not np.isnan(train_df.get_value(i, 'domain2_score')):\n",
    "        assert train_df.get_value(i, 'essay_set') == 2\n",
    "        new_val = train_df.get_value(i, 'domain1_score') + train_df.get_value(i, 'domain2_score')\n",
    "        train_df.set_value(i, 'domain1_score', new_val) \n",
    "train_df = train_df.drop('domain2_score', axis=1)\n",
    "train_df = train_df.rename(columns={'domain1_score': 'score'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_str(string):\n",
    "  \n",
    "    string = re.sub(r\"[^A-Za-z0-9(),.!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    #string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "\n",
    "    return string.strip().lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(train_df)):\n",
    "    train_df.essay[i]=clean_str(train_df.essay[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "regularization_data = create_regularization_data(train_df)\n",
    "train_df = append_regularized_scores(train_df)\n",
    "\n",
    "#validate that the standardization works\n",
    "max_essay_set = max(train_df['essay_set'])\n",
    "for i in range (max_essay_set):\n",
    "    valid = train_df[train_df[\"essay_set\"] == i + 1][\"std_score\"]\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in validation data\n",
    "valid_cols = ['essay_id', 'essay_set', 'essay', 'domain1_predictionid', 'domain2_predictionid']\n",
    "valid_df = pd.read_csv('valid_set.tsv', delimiter='\\t', usecols=valid_cols,encoding='iso-8859-1')\n",
    "valid_df['score'] = pd.Series([0] * valid_df.shape[0], index=valid_df.index)\n",
    "\n",
    "# scores are stored in separate data set, we'll put them in same one\n",
    "valid_scores = pd.read_csv('valid_sample_submission_5_column.csv', delimiter=',')\n",
    "\n",
    "# put each score in our data set, and make sure to handle essay set 2\n",
    "for i in range(valid_df.shape[0]):\n",
    "    dom1_predid = valid_df.get_value(i, 'domain1_predictionid')\n",
    "    row = valid_scores[valid_scores['prediction_id'] == dom1_predid]\n",
    "    score = row.get_value(row.index[0], 'predicted_score')\n",
    "    \n",
    "    dom2_predid = valid_df.get_value(i, 'domain2_predictionid')\n",
    "    if not np.isnan(dom2_predid):\n",
    "        assert valid_df.get_value(i, 'essay_set') == 2\n",
    "        rowB = valid_scores[valid_scores['prediction_id'] == dom2_predid]\n",
    "        scoreB = rowB.get_value(rowB.index[0], 'predicted_score')\n",
    "        score += scoreB\n",
    "        \n",
    "    valid_df.set_value(i, 'score', score)\n",
    "        \n",
    "valid_df = valid_df.drop(['domain1_predictionid', 'domain2_predictionid'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(valid_df)):\n",
    "    valid_df.essay[i]=clean_str(valid_df.essay[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_standardized_column(train_df, valid_df, non_std_col_name):\n",
    "    std_col_name = \"std_\" + non_std_col_name\n",
    "    train_df = append_zeros_column(train_df, std_col_name)\n",
    "    valid_df = append_zeros_column(valid_df, std_col_name)\n",
    "\n",
    "    std_data = create_standardization_data(train_df, non_std_col_name)\n",
    "\n",
    "    dfs = [train_df, valid_df]\n",
    "    for df in dfs:\n",
    "        for i in range(df.shape[0]):\n",
    "            essay_set = df.get_value(i, 'essay_set')\n",
    "            non_std_val = df.get_value(i, non_std_col_name)\n",
    "            if std_data[essay_set - 1][0] < std_data[essay_set - 1][1]:\n",
    "                df = df.set_value(i, std_col_name, (non_std_val - std_data[essay_set - 1][0]) / std_data[essay_set - 1][1])\n",
    "            else:\n",
    "                df=df.set_value(i, std_col_name,non_std_val)\n",
    "            \n",
    "    return train_df,valid_df\n",
    "                \n",
    "           \n",
    "                    \n",
    "# returns a column to place data\n",
    "def append_zeros_column(df, title):\n",
    "    df[title] = pd.Series([0.0] * df.shape[0], index=df.index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_standardization_data(train_df, column_name):\n",
    "    #getting the number of datasets\n",
    "    max_essay_set = max(train_df['essay_set'])\n",
    "    #list of the standardized values\n",
    "    standardization_data = []\n",
    "    for i in range(1, max_essay_set+1):\n",
    "        mean = np.mean((train_df[train_df['essay_set'] == i])[column_name])\n",
    "        std = np.std((train_df[train_df['essay_set'] == i])[column_name])\n",
    "        standardization_data.append([mean, std])\n",
    "    return standardization_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COUNTING THE NUMBER OF UNIQUE WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "def fill_unique_words_column(train_df, valid_df):\n",
    "\n",
    "    #percentage of unique words to the total number of words\n",
    "    unique_word_percentages_train = []\n",
    "    unique_word_percentages_valid = []\n",
    "\n",
    "    for i in range(len(train_df)):\n",
    "        splits = train_df.iloc[i][\"essay\"].split()\n",
    "        total_words = len(splits)\n",
    "        unique_words = len(Counter(splits))\n",
    "        percentage = float(unique_words) / total_words\n",
    "        unique_word_percentages_train.append(percentage)\n",
    "        \n",
    "    for i in range(len(valid_df)):\n",
    "        splits = valid_df.iloc[i][\"essay\"].split()\n",
    "        total_words = len(splits)\n",
    "        unique_words = len(Counter(splits))\n",
    "        percentage = float(unique_words) / total_words\n",
    "        unique_word_percentages_valid.append(percentage)    \n",
    "\n",
    "    #Add the features to the dataset\n",
    "    train_df[\"unique_words\"] = unique_word_percentages_train\n",
    "    valid_df[\"unique_words\"] = unique_word_percentages_valid\n",
    "\n",
    "    train_df, valid_df = append_standardized_column(train_df, valid_df, 'unique_words')\n",
    "\n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fill_unique_words_column(train_df, valid_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://textblob.readthedocs.io/en/dev/quickstart.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['sentiment']=float(0)\n",
    "valid_df['sentiment']=float(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(train_df)):\n",
    "    testimonial=TextBlob(train_df.essay[i])\n",
    "    train_df['sentiment'][i]=testimonial.sentiment.polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(valid_df)):\n",
    "    testimonial=TextBlob(valid_df.essay[i])\n",
    "    valid_df.sentiment[i]=testimonial.sentiment.polarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence count and word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['sent_len']=0\n",
    "train_df['word_count']=0\n",
    "valid_df['sent_len']=0\n",
    "valid_df['word_count']=0\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(train_df)):\n",
    "    zen=TextBlob(train_df.essay[i])\n",
    "    train_df['word_count'][i]=len(zen.words)\n",
    "    train_df['sent_len'][i]=len(zen.sentences)\n",
    "    \n",
    "for i in range(0,len(valid_df)):\n",
    "    zen=TextBlob(valid_df.essay[i])\n",
    "    valid_df['word_count'][i]=len(zen.words)\n",
    "    valid_df['sent_len'][i]=len(zen.sentences)                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling Correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['mistake_count']=0\n",
    "valid_df['mistake_count']=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(train_df)):\n",
    "    word=list(train_df.essay[i])\n",
    "    wrong=0\n",
    "    for j in range(0,len(word)):\n",
    "        if word[j]!=TextBlob(word[j]).correct():\n",
    "            wrong=wrong+1\n",
    "    train_df.mistake_count[i]=wrong\n",
    "for i in range(0,len(valid_df)):\n",
    "    word=list(valid_df.essay[i])\n",
    "    wrong=0\n",
    "    for j in range(0,len(word)):\n",
    "        if word[j]!=TextBlob(word[j]).correct():\n",
    "            wrong=wrong+1\n",
    "    valid_df.mistake_count[i]=wrong            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar mistake check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import language_check\n",
    "tool = language_check.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['gramm_mistake']=0\n",
    "valid_df['gramm_mistake']=0\n",
    "import grammar_check\n",
    "tool = grammar_check.LanguageTool('en-GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(train_df)):\n",
    "    matches=tool.check(train_df.essay[i])\n",
    "    train_df.gramm_mistake[i]=len(matches)\n",
    "\n",
    "for i in range(0,len(valid_df)):\n",
    "    matches=tool.check(valid_df.essay[i])\n",
    "    valid_df.gramm_mistake[i]=len(matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Noun count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['noun_count']=0\n",
    "valid_df['noun_count']=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(train_df)):\n",
    "    zen=TextBlob(train_df.essay[i])\n",
    "    train_df['noun_count'][i]=len(zen.np_counts)\n",
    "for i in range(0,len(valid_df)):\n",
    "    zen=TextBlob(valid_df.essay[i])\n",
    "    valid_df['noun_count'][i]=len(zen.np_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# avg word length and avg setence length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['avg_word_len']=float(0)\n",
    "valid_df['avg_word_len']=float(0)\n",
    "\n",
    "train_df['avg_sent_len']=float(0)\n",
    "valid_df['avg_sent_len']=float(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(train_df)):\n",
    "#     word_len=[]\n",
    "    sent_len=[]\n",
    "    zen=TextBlob(train_df.essay[i])\n",
    "#     for i in range(0,len(zen.words)):\n",
    "#         word_len.append(len(zen.words[i]))\n",
    "#     train_df['avg_word_len'][i]=sum(word_len)/len(word_len)\n",
    "    for j in range(0,len(zen.sentences)):\n",
    "        sent_len.append(len(zen.sentences[j]))\n",
    "    train_df['avg_sent_len'][i]=sum(sent_len)/len(sent_len)\n",
    "    \n",
    "    \n",
    "    \n",
    "for i in range(0,len(valid_df)):\n",
    "#     word_len=[]\n",
    "    sent_len=[]\n",
    "    zen=TextBlob(valid_df.essay[i])\n",
    "#     for i in range(0,len(zen.words)):\n",
    "#         word_len.append(len(zen.words[i]))\n",
    "#     valid_df['avg_word_len'][i]=sum(word_len)/len(word_len)\n",
    "    for j in range(0,len(zen.sentences)):\n",
    "        sent_len.append(len(zen.sentences[j]))\n",
    "    valid_df['avg_sent_len'][i]=sum(sent_len)/len(sent_len)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long word length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['long_word_count']=0\n",
    "valid_df['long_word_count']=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(train_df)):\n",
    "    long_word=0\n",
    "    zen=TextBlob(train_df.essay[i])\n",
    "    for i in range(0,len(zen.words)):\n",
    "        if len(zen.words[i])>7:\n",
    "            long_word=long_word+1\n",
    "    train_df['long_word_count'][i]=long_word\n",
    "    \n",
    "    \n",
    "    \n",
    "for i in range(0,len(valid_df)):\n",
    "    long_word=0\n",
    "    zen=TextBlob(valid_df.essay[i])\n",
    "    for i in range(0,len(zen.words)):\n",
    "        if len(zen.words[i])>7:\n",
    "            long_word=long_word+1\n",
    "    valid_df['long_word_count'][i]=long_word\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab richness\n",
    "https://swizec.com/blog/measuring-vocabulary-richness-with-python/swizec/2528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['vocab_richness']=float(0)\n",
    "valid_df['vocab_richness']=float(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(train_df)):\n",
    "    long_word=0\n",
    "    zen=TextBlob(train_df.essay[i])\n",
    "    M1=len(zen.words)\n",
    "    m2=[]\n",
    "    values, counts = np.unique(zen.words, return_counts=True)\n",
    "    for i in range(0,len(values)):\n",
    "        m2.append(counts[i]^2)\n",
    "    M2=sum(m2)\n",
    "    try:\n",
    "        train_df.vocab_richness[i]=(M1*M1)/(M2-M1)\n",
    "    except:\n",
    "        train_df.vocab_richness[i]=0\n",
    "        \n",
    "for i in range(0,len(valid_df)):\n",
    "    long_word=0\n",
    "    zen=TextBlob(valid_df.essay[i])\n",
    "    M1=len(zen.words)\n",
    "    m2=[]\n",
    "    values, counts = np.unique(zen.words, return_counts=True)\n",
    "    for i in range(0,len(values)):\n",
    "        m2.append(counts[i]^2)\n",
    "    M2=sum(m2)\n",
    "    try:\n",
    "        valid_df.vocab_richness[i]=(M1*M1)/(M2-M1)\n",
    "    except:\n",
    "        valid_df.vocab_richness[i]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# proper noun and adjective count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wiki = TextBlob(\"Python is a high-level, general-purpose programming language.\")\n",
    "# list(wiki.tags[0])\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "train_df['proper_noun_count']=0\n",
    "valid_df['proper_noun_count']=0\n",
    "\n",
    "train_df['adj_count']=0\n",
    "valid_df['adj_count']=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(train_df)):\n",
    "    zen=list(TextBlob(train_df.essay[i]).tags)\n",
    "    train_df['proper_noun_count'][i]=len([x[0] for x in zen if 'NNP'==x[1] or 'NNPS'== x[1]])\n",
    "    train_df['adj_count'][i]=len([x[0] for x in zen if 'JJR'==x[1] or 'JJS'== x[1]])\n",
    "    \n",
    "\n",
    "    \n",
    "for i in range(0,len(valid_df)):\n",
    "    zen=list(TextBlob(valid_df.essay[i]).tags)\n",
    "    valid_df['proper_noun_count'][i]=len([x[0] for x in zen if 'NNP'==x[1] or 'NNPS'== x[1]])\n",
    "    valid_df['adj_count'][i]=len([x[0] for x in zen if 'JJR'==x[1] or 'JJS'== x[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['word_count', 'noun_count',\n",
    "       'avg_word_len', 'avg_sent_len', 'long_word_count', 'vocab_richness',\n",
    "       'proper_noun_count', 'adj_count','sent_len']:\n",
    "    append_standardized_column(train_df, valid_df, i)\n",
    "train_df = train_df.drop(['word_count', 'essay','essay_id','Unnamed: 0','unique_words','noun_count', \n",
    "       'avg_word_len', 'avg_sent_len', 'long_word_count', 'vocab_richness',\n",
    "       'proper_noun_count', 'adj_count'], axis=1)\n",
    "valid_df = valid_df.drop(['word_count','essay','Unnamed: 0','essay','essay_id','unique_words', 'noun_count',\n",
    "       'avg_word_len', 'avg_sent_len', 'long_word_count', 'vocab_richness',\n",
    "       'proper_noun_count', 'adj_count'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=train_df[['essay_set', 'score', 'std_score', 'std_unique_words', 'sentiment',\n",
    "        'std_word_count', 'std_noun_count', 'std_avg_word_len',\n",
    "       'std_avg_sent_len', 'std_long_word_count', 'std_vocab_richness',\n",
    "       'std_proper_noun_count', 'std_adj_count', 'std_sent_len']]\n",
    "valid=valid_df[['essay_set', 'score', 'std_unique_words', 'sentiment',\n",
    "        'std_word_count', 'std_noun_count', 'std_avg_word_len',\n",
    "       'std_avg_sent_len', 'std_long_word_count', 'std_vocab_richness',\n",
    "       'std_proper_noun_count', 'std_adj_count', 'std_sent_len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "  \n",
    "# create a regressor object \n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=6, random_state=0)  \n",
    "regressor.fit(train.drop(['score', 'std_score'],axis=1), train.std_score)  \n",
    "y_pred = regressor.predict(valid.drop('score',axis=1))  \n",
    "valid_df[\"Log_L2 predicted_scores\"] =y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_pred_values_l1 = []\n",
    "for i in range(8):\n",
    "    current_set = valid_df[valid_df['essay_set'] == i + 1]['Log_L2 predicted_scores']\n",
    "    for value in current_set:\n",
    "        stand_pred_values_l1.append(int(float(value) * float(regularization_data[i][2]) + (regularization_data[i][1])))\n",
    "# print stand_pred_values_l1\n",
    "\n",
    "#adding the denormalizede predicted values to the valid_df dataset\n",
    "valid_df['newly_predicted_scores_log_l2'] = stand_pred_values_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist=[]\n",
    "for i in range(0,len(valid_df)):\n",
    "    mylist.append(abs(valid_df['score'][i]-valid_df['newly_predicted_scores_log_l2'][i]))\n",
    "uniq, counts = np.unique(mylist, return_counts=True)\n",
    "pd.DataFrame({'Difference':uniq,'Counts':counts})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import metrics\n",
    "gbrt=GradientBoostingRegressor(loss='ls', learning_rate=0.1,n_estimators=100,max_depth=3)\n",
    "gbrt.fit(train.drop(['score', 'std_score'],axis=1), train.std_score) \n",
    "y_pred=gbrt.predict(valid.drop('score',axis=1))\n",
    "valid_df[\"Log_L2 predicted_scores\"] =y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_pred_values_l1 = []\n",
    "for i in range(8):\n",
    "    current_set = valid_df[valid_df['essay_set'] == i + 1]['Log_L2 predicted_scores']\n",
    "    for value in current_set:\n",
    "        stand_pred_values_l1.append(int(float(value) * float(regularization_data[i][2]) + (regularization_data[i][1])))\n",
    "# print stand_pred_values_l1\n",
    "\n",
    "#adding the denormalizede predicted values to the valid_df dataset\n",
    "valid_df['newly_predicted_scores_log_l2'] = stand_pred_values_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist=[]\n",
    "for i in range(0,len(valid_df)):\n",
    "    mylist.append(abs(valid_df['score'][i]-valid_df['newly_predicted_scores_log_l2'][i]))\n",
    "uniq, counts = np.unique(mylist, return_counts=True)\n",
    "pd.DataFrame({'Difference':uniq,'Counts':counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
